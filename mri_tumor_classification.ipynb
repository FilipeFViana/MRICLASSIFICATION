{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANDO BIBLIOTECAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIGURAÇÕES GERAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações gerais\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANDO DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos do dataset\n",
    "dataset_root = './DataSet'\n",
    "training_dir = os.path.join(dataset_root, 'Training')\n",
    "validation_dir = os.path.join(dataset_root, 'Validation')\n",
    "testing_dir = os.path.join(dataset_root, 'Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMAÇÃO DAS IMAGENS, CARREGAMENTO DO DATASET E CRIAÇÃO DOS DATA LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformações para as imagens\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val_test': transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Carregar datasets\n",
    "datasets_dict = {\n",
    "    'train': datasets.ImageFolder(root=training_dir, transform=transform['train']),\n",
    "    'val': datasets.ImageFolder(root=validation_dir, transform=transform['val_test']),\n",
    "    'test': datasets.ImageFolder(root=testing_dir, transform=transform['val_test'])\n",
    "}\n",
    "\n",
    "# Criar DataLoaders\n",
    "def create_dataloaders(batch_size):\n",
    "    return {\n",
    "        'train': DataLoader(datasets_dict['train'], batch_size=batch_size, shuffle=True),\n",
    "        'val': DataLoader(datasets_dict['val'], batch_size=batch_size, shuffle=False),\n",
    "        'test': DataLoader(datasets_dict['test'], batch_size=batch_size, shuffle=False)\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIGURAÇÃO DOS MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar modelos\n",
    "num_classes = len(datasets_dict['train'].classes)\n",
    "\n",
    "resnet_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, num_classes)\n",
    "\n",
    "# DenseNet121\n",
    "densenet_model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "densenet_model.classifier = nn.Linear(densenet_model.classifier.in_features, num_classes)\n",
    "\n",
    "# Inception v3\n",
    "inception_model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT, aux_logits=True)\n",
    "inception_model.fc = nn.Linear(inception_model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNÇÃO DE TREINAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de treinamento\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, model_name=\"model\"):\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Treinando - Epoch {epoch+1}/{epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if isinstance(outputs, tuple):  # Para Inception v3\n",
    "                main_output, aux_output = outputs\n",
    "                loss = criterion(main_output, labels) + 0.4 * criterion(aux_output, labels)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs[0] if isinstance(outputs, tuple) else outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validando\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    main_output = outputs[0]\n",
    "                    loss = criterion(main_output, labels)\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(main_output if isinstance(outputs, tuple) else outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }, f\"{model_name}_metrics.pth\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNÇÃO DE AVALIAÇÃO E PLOTAGEM DA MATRIZ DE CONFUSÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de avaliação e matriz de confusão\n",
    "def evaluate_model(model, test_loader, model_name=\"model\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]  # Usar apenas a saída principal\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='weighted')\n",
    "    rec = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=datasets_dict['train'].classes)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Matriz de Confusão - {model_name}\")\n",
    "    plt.savefig(f\"{model_name}_confusion_matrix.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TREINO E AVALIAÇÃO DO MODELO RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar e avaliar ResNet\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(resnet_model.parameters(), lr=0.001)\n",
    "trained_resnet = train_model(resnet_model, dataloaders['train'], dataloaders['val'], criterion, optimizer, epochs=5, model_name=\"ResNet18\")\n",
    "evaluate_model(trained_resnet, dataloaders['test'], model_name=\"ResNet18\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TREINO E AVALIAÇÃO DO MODELO DENSENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar e avaliar DenseNet\n",
    "optimizer = Adam(densenet_model.parameters(), lr=0.001)\n",
    "trained_densenet = train_model(densenet_model, dataloaders['train'], dataloaders['val'], criterion, optimizer, epochs=5, model_name=\"DenseNet121\")\n",
    "evaluate_model(trained_densenet, dataloaders['test'], model_name=\"DenseNet121\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TREINO E AVALIAÇÃO DO MODELO INCEPTION V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Treinar e avaliar Inception\n",
    "optimizer = Adam(inception_model.parameters(), lr=0.001)\n",
    "trained_inception = train_model(inception_model, dataloaders['train'], dataloaders['val'], criterion, optimizer, epochs=5, model_name=\"InceptionV3\")\n",
    "evaluate_model(trained_inception, dataloaders['test'], model_name=\"InceptionV3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
